{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "593d57ce",
   "metadata": {},
   "source": [
    "\n",
    "# Live LLaMA Architecture Sandbox\n",
    "\n",
    "This notebook instantiates the exact `LiveLlamaForCausalLM` architecture from the repository and exercises it with dummy multimodal batches. The goal is to trace how text tokens, frame placeholders, and dense frame descriptors flow through the connector, the joint embedding, and the causal LM head without relying on any auxiliary wrappers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4507db64",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment setup\n",
    "\n",
    "We import the production modules directly from the repository and fix the random seed for reproducibility.  Printing happens aggressively throughout the notebook so we can inspect intermediate tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd8805f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from models.live_llama import LiveLlamaConfig, LiveLlamaForCausalLM\n",
    "\n",
    "SEED = 17\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Running on device: {torch.device('cpu')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c7d1be",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Build a tiny but faithful Live LLaMA\n",
    "\n",
    "We reuse the production configuration class and only shrink hidden sizes / vocabulary to keep the demo lightweight.  All architectural pieces (token embeddings, rotary attention, connector MLP, etc.) remain exactly the same as the real model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9891c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The config mirrors the real Live LLaMA setup while shrinking dimensions for CPU-friendly experimentation.\n",
    "config = LiveLlamaConfig(\n",
    "    vocab_size=160,\n",
    "    hidden_size=128,\n",
    "    intermediate_size=256,\n",
    "    num_hidden_layers=2,\n",
    "    num_attention_heads=4,\n",
    "    num_key_value_heads=4,\n",
    "    max_position_embeddings=512,\n",
    "    bos_token_id=1,\n",
    "    eos_token_id=2,\n",
    "    pad_token_id=0,\n",
    "    # Live-specific knobs\n",
    "    vision_hidden_size=96,\n",
    "    frame_token_cls=True,\n",
    "    frame_token_pooled=[2, 2],  # 1 cls + 2x2 spatial tokens => 5 placeholders per frame\n",
    "    frame_num_tokens=5,\n",
    "    v_placeholder_id=158,\n",
    "    frame_token_interval_id=159,\n",
    "    stream_loss_weight=3.0,\n",
    ")\n",
    "model = LiveLlamaForCausalLM(config)\n",
    "print(model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6173c99a",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Dummy dataset that matches the production contract\n",
    "\n",
    "Each sample returns:\n",
    "\n",
    "* `input_ids`: token ids containing regular text, interval separators, and visual placeholders.\n",
    "* `frames`: dense frame descriptors with shape `(num_frames, frame_num_tokens, vision_hidden_size)`.\n",
    "* `labels`: targets for causal LM training where system / user prompt tokens are masked out with `-100`.\n",
    "\n",
    "The logic mirrors how the training pipeline expects data to be structured.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af646d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DummyConversation:\n",
    "    text_span: list[int]\n",
    "    num_frames: int\n",
    "\n",
    "\n",
    "class LiveLikeDataset(Dataset):\n",
    "    def __init__(self, *, conversations, max_prompt_tokens=12):\n",
    "        self.conversations = conversations\n",
    "        self.max_prompt_tokens = max_prompt_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "\n",
    "    def _build_sequence(self, convo: DummyConversation):\n",
    "        # System prompt + user prompt tokens (masked in the loss)\n",
    "        prompt_tokens = [config.bos_token_id]\n",
    "        prompt_tokens += random.sample(convo.text_span, k=min(len(convo.text_span), self.max_prompt_tokens))\n",
    "        # Assistant response prefix before frames arrive (will be trained)\n",
    "        response_prefix = [random.choice(convo.text_span) for _ in range(3)]\n",
    "\n",
    "        sequence = prompt_tokens + response_prefix\n",
    "        for frame_idx in range(convo.num_frames):\n",
    "            sequence.append(config.frame_token_interval_id)\n",
    "            frame_placeholder_slice = [config.v_placeholder_id] * config.frame_num_tokens\n",
    "            sequence.extend(frame_placeholder_slice)\n",
    "            # Optionally interleave actual response tokens after each frame chunk\n",
    "            sequence.append(random.choice(convo.text_span))\n",
    "        sequence.append(config.eos_token_id)\n",
    "        return sequence, prompt_tokens\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        convo = self.conversations[idx]\n",
    "        token_sequence, prompt_tokens = self._build_sequence(convo)\n",
    "        input_ids = torch.tensor(token_sequence, dtype=torch.long)\n",
    "        labels = input_ids.clone()\n",
    "        # Mask out prompt tokens so the model only learns on assistant responses + frame slots\n",
    "        labels[: len(prompt_tokens)] = -100\n",
    "        frames = torch.randn(convo.num_frames, config.frame_num_tokens, config.vision_hidden_size)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"frames\": frames,\n",
    "        }\n",
    "\n",
    "\n",
    "def live_like_collate(batch):\n",
    "    input_ids = nn.utils.rnn.pad_sequence([sample[\"input_ids\"] for sample in batch], batch_first=True, padding_value=config.pad_token_id)\n",
    "    labels = nn.utils.rnn.pad_sequence([sample[\"labels\"] for sample in batch], batch_first=True, padding_value=-100)\n",
    "    frames_per_sample = [sample[\"frames\"] for sample in batch]\n",
    "    if frames_per_sample:\n",
    "        frames = torch.cat(frames_per_sample, dim=0)\n",
    "        frame_counts = torch.tensor([item.shape[0] for item in frames_per_sample], dtype=torch.long)\n",
    "    else:\n",
    "        frames = torch.zeros(0, config.frame_num_tokens, config.vision_hidden_size)\n",
    "        frame_counts = torch.zeros(0, dtype=torch.long)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "        \"frames\": frames,\n",
    "        \"frame_counts\": frame_counts,\n",
    "    }\n",
    "\n",
    "\n",
    "vocab_pool = list(range(10, 150))\n",
    "conversations = [\n",
    "    DummyConversation(text_span=vocab_pool, num_frames=2),\n",
    "    DummyConversation(text_span=vocab_pool, num_frames=3),\n",
    "    DummyConversation(text_span=vocab_pool, num_frames=1),\n",
    "]\n",
    "\n",
    "dataset = LiveLikeDataset(conversations=conversations)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=False, collate_fn=live_like_collate)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "for idx in range(len(dataset)):\n",
    "    sample = dataset[idx]\n",
    "    print(f\"Sample {idx} -> tokens: {sample['input_ids'].tolist()}\")\n",
    "    print(f\"Sample {idx} -> labels: {sample['labels'].tolist()}\")\n",
    "    print(f\"Sample {idx} -> frames shape: {tuple(sample['frames'].shape)}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d32a2a0",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Inspect a batch flowing through the connector and joint embedding\n",
    "\n",
    "We take the first batch from the loader, push it through `joint_embed`, and show every mask / tensor shape the real training step would touch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8e4c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"input_ids shape: {tuple(batch['input_ids'].shape)}\")\n",
    "print(f\"labels shape: {tuple(batch['labels'].shape)}\")\n",
    "print(f\"frames shape: {tuple(batch['frames'].shape)} (total_frames, tokens_per_frame, hidden)\")\n",
    "print(f\"frame_counts per sample: {batch['frame_counts'].tolist()}\")\n",
    "print(f\"Total frames represented: {int(batch['frame_counts'].sum())}\")\n",
    "\n",
    "v_mask = batch['input_ids'] == config.v_placeholder_id\n",
    "print(f\"Visual placeholder mask (per token): {v_mask}\")\n",
    "num_visual_tokens = int(v_mask.sum())\n",
    "print(f\"Total visual tokens expected from placeholders: {num_visual_tokens}\")\n",
    "print(f\"Total tokens supplied by frames tensor: {batch['frames'].shape[0] * config.frame_num_tokens}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    embedded = model.joint_embed(batch['input_ids'], batch['frames'])\n",
    "print(f\"joint_embed output shape: {tuple(embedded.shape)}\")\n",
    "print(f\"First 5 embedded token norms: {embedded.norm(dim=-1)[:5]}\")\n",
    "\n",
    "reshaped_frames = batch['frames'].view(-1, config.frame_num_tokens, config.vision_hidden_size)\n",
    "print(f\"Connector input view shape (per frame): {tuple(reshaped_frames.shape)}\")\n",
    "print(f\"Connector MLP weights summary: {[tuple(layer.weight.shape) for layer in model.connector if hasattr(layer, 'weight')]}\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79285ef5",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Verbose training loop\n",
    "\n",
    "The loop runs a couple of iterations, printing:\n",
    "\n",
    "* Token-level loss weights (text vs. stream tokens)\n",
    "* The causal LM logits tensor shape\n",
    "* Gradient statistics for the connector and language head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4752ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
    "\n",
    "for step, batch in enumerate(dataloader):\n",
    "    frame_counts = batch.pop('frame_counts')\n",
    "    total_frame_tokens = int(frame_counts.sum().item() * config.frame_num_tokens) if frame_counts.numel() else 0\n",
    "    print(f\"\\n=== Step {step} ===\")\n",
    "    print(f\"Frame counts in batch: {frame_counts.tolist()} -> {total_frame_tokens} frame placeholders\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    logits = outputs.logits\n",
    "\n",
    "    v_mask = (batch['input_ids'] == config.v_placeholder_id)\n",
    "    learn_mask = batch['labels'] != -100\n",
    "    stream_mask = v_mask & learn_mask\n",
    "    text_mask = learn_mask & ~v_mask\n",
    "\n",
    "    print(f\"Loss: {loss.item():.4f}\")\n",
    "    print(f\"Logits shape: {tuple(logits.shape)} (batch, seq, vocab)\")\n",
    "    print(f\"Num learnable tokens -> text: {int(text_mask.sum())}, stream: {int(stream_mask.sum())}\")\n",
    "    if stream_mask.any():\n",
    "        print(f\"Example stream token logits (first stream token): {logits[stream_mask][0][:5]}\")\n",
    "\n",
    "    loss.backward()\n",
    "    connector_grad_norm = torch.norm(torch.stack([p.grad.norm() for p in model.connector.parameters() if p.grad is not None]))\n",
    "    lm_head_grad_norm = model.lm_head.weight.grad.norm()\n",
    "    print(f\"Connector grad norm: {connector_grad_norm:.4f}\")\n",
    "    print(f\"LM head grad norm: {lm_head_grad_norm:.4f}\")\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if step >= 1:\n",
    "        break\n",
    ""
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}