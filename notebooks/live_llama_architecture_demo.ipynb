{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c14405d",
   "metadata": {},
   "source": [
    "# VideoLLM Architecture Playground\n",
    "\n",
    "This notebook reconstructs the high-level components of the VideoLLM live streaming architecture in a lightweight, fully self-contained form. It mirrors the key classes in [`models/modeling_live.py`](../models/modeling_live.py) and [`models/live_llama/modeling_live_llama.py`](../models/live_llama/modeling_live_llama.py) by providing:\n",
    "\n",
    "* a joint text-video embedding step with a connector that projects vision features into the language model hidden size,\n",
    "* a minimal causal language model core that consumes the fused embeddings, and\n",
    "* a training loss that up-weights the contribution of video placeholder tokens, just like the original implementation's `stream_loss_weight`.\n",
    "\n",
    "To make the inner workings visible, the notebook injects extensive print statements so that tensor shapes and intermediate values are surfaced at every major step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6176c89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aa150a",
   "metadata": {},
   "source": [
    "## 1. Configuration scaffold\n",
    "\n",
    "The real project stores configuration values in `LiveConfigMixin` and `LiveLlamaConfig`.\n",
    "Here we define a tiny dataclass-like container that captures only the attributes that the model relies on during forward passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf33284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyLiveConfig:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int = 64,\n",
    "        hidden_size: int = 32,\n",
    "        vision_hidden_size: int = 24,\n",
    "        frame_num_tokens: int = 1,\n",
    "        stream_loss_weight: float = 3.0,\n",
    "        v_placeholder_id: int = 63,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vision_hidden_size = vision_hidden_size\n",
    "        self.frame_num_tokens = frame_num_tokens\n",
    "        self.stream_loss_weight = stream_loss_weight\n",
    "        self.v_placeholder_id = v_placeholder_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4daa42",
   "metadata": {},
   "source": [
    "## 2. Minimal causal language backbone\n",
    "\n",
    "The production model subclasses `LlamaForCausalLM`. To keep things lightweight we implement a tiny GRU-based causal language model that exposes the same helper methods used by the mixin (embedding lookup and a linear language head)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d13128",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyCausalBackbone(nn.Module):\n",
    "    def __init__(self, config: TinyLiveConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.rnn = nn.GRU(config.hidden_size, config.hidden_size, batch_first=True)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size)\n",
    "\n",
    "    def forward(self, inputs_embeds: torch.Tensor):\n",
    "        print(f\"[TinyCausalBackbone] inputs_embeds shape: {inputs_embeds.shape}\")\n",
    "        hidden_states, _ = self.rnn(inputs_embeds)\n",
    "        print(f\"[TinyCausalBackbone] hidden_states shape: {hidden_states.shape}\")\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        print(f\"[TinyCausalBackbone] logits shape: {logits.shape}\")\n",
    "        return logits\n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embed_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5003c6e",
   "metadata": {},
   "source": [
    "## 3. Live-style fusion module\n",
    "\n",
    "The heart of the architecture lives in `LiveMixin` and `LiveLlamaForCausalLM`.\n",
    "The next cell recreates their behavior:\n",
    "\n",
    "* `visual_embed` processes video frame features through a connector (two linear layers with GELU) and flattens them into token embeddings.\n",
    "* `joint_embed` splices the resulting video embeddings into the token stream wherever the video placeholder token appears.\n",
    "* `forward` calls the backbone, applies the language head, and computes the weighted cross-entropy loss that the original model uses when `stream_loss_weight != 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyLiveLlama(nn.Module):\n",
    "    def __init__(self, config: TinyLiveConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.backbone = TinyCausalBackbone(config)\n",
    "        self.connector = nn.Sequential(\n",
    "            nn.Linear(config.vision_hidden_size, config.hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "        )\n",
    "\n",
    "    def visual_embed(self, frames: torch.Tensor) -> torch.Tensor:\n",
    "        print(f\"[visual_embed] raw frames shape: {frames.shape}\")\n",
    "        batch, num_frames, feat_dim = frames.shape\n",
    "        flat_frames = frames.view(-1, feat_dim)\n",
    "        projected = self.connector(flat_frames)\n",
    "        print(f\"[visual_embed] projected shape: {projected.shape}\")\n",
    "        token_embeddings = projected.view(batch, num_frames * self.config.frame_num_tokens, -1)\n",
    "        print(f\"[visual_embed] token_embeddings shape: {token_embeddings.shape}\")\n",
    "        return token_embeddings\n",
    "\n",
    "    def joint_embed(self, input_ids: torch.Tensor, frames: torch.Tensor | None) -> torch.Tensor:\n",
    "        text_embeds = self.backbone.get_input_embeddings()(input_ids)\n",
    "        print(f\"[joint_embed] text_embeds shape: {text_embeds.shape}\")\n",
    "        if frames is None:\n",
    "            return text_embeds\n",
    "        video_tokens = self.visual_embed(frames)\n",
    "        batch, seq_len, hidden = text_embeds.shape\n",
    "        video_tokens = video_tokens.view(batch, -1, hidden)\n",
    "        v_mask = input_ids == self.config.v_placeholder_id\n",
    "        print(f\"[joint_embed] video token count per sample: {v_mask.sum(dim=1)}\")\n",
    "        if not torch.all(v_mask.sum(dim=1) == video_tokens.size(1)):\n",
    "            raise ValueError(\"Number of video placeholders must match provided frame tokens\")\n",
    "        expanded = text_embeds.clone()\n",
    "        expanded[v_mask] = video_tokens.view(-1, hidden)\n",
    "        print(f\"[joint_embed] expanded embeddings shape: {expanded.shape}\")\n",
    "        return expanded\n",
    "\n",
    "    def forward(self, input_ids: torch.Tensor, frames: torch.Tensor | None = None, labels: torch.Tensor | None = None):\n",
    "        print(\"\n",
    "========== Forward Pass ==========\")\n",
    "        print(f\"input_ids shape: {input_ids.shape}\")\n",
    "        if frames is not None:\n",
    "            print(f\"frames shape: {frames.shape}\")\n",
    "        embeddings = self.joint_embed(input_ids, frames)\n",
    "        logits = self.backbone(embeddings)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            print(f\"labels shape: {labels.shape}\")\n",
    "            flat_logits = logits.view(-1, self.config.vocab_size)\n",
    "            flat_labels = labels.view(-1)\n",
    "            valid_mask = flat_labels != -100\n",
    "            per_token_loss = nn.functional.cross_entropy(\n",
    "                flat_logits, flat_labels, ignore_index=-100, reduction='none'\n",
    "            )\n",
    "            weight = torch.ones_like(per_token_loss)\n",
    "            placeholder_mask = (input_ids.view(-1) == self.config.v_placeholder_id) & valid_mask\n",
    "            weight[placeholder_mask] = self.config.stream_loss_weight\n",
    "            weighted_loss = (per_token_loss * weight * valid_mask.float()).sum() / valid_mask.sum().clamp_min(1)\n",
    "            print(f\"[loss] mean loss: {weighted_loss.item():.4f}\")\n",
    "            print(f\"[loss] placeholder_mask sum: {placeholder_mask.sum().item()}\")\n",
    "            loss = weighted_loss\n",
    "\n",
    "        return {\"logits\": logits, \"loss\": loss}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c6736b",
   "metadata": {},
   "source": [
    "## 4. Dummy dataset and dataloader\n",
    "\n",
    "We now create synthetic samples that contain both text token IDs and slots for video frames.\n",
    "Each sample mimics a `[text, <v>, <v>, text]` pattern with corresponding labels and two frame feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c708ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyVideoTextDataset(Dataset):\n",
    "    def __init__(self, config: TinyLiveConfig, num_samples: int = 4, seq_len: int = 8, num_frames: int = 2):\n",
    "        self.config = config\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.num_frames = num_frames\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.randint(3, self.config.vocab_size - 2, (self.seq_len,), dtype=torch.long)\n",
    "        placeholder_positions = torch.randperm(self.seq_len)[: self.num_frames]\n",
    "        input_ids[placeholder_positions] = self.config.v_placeholder_id\n",
    "\n",
    "        labels = input_ids.roll(-1)\n",
    "        labels[-1] = -100\n",
    "\n",
    "        frames = torch.randn(self.num_frames, self.config.vision_hidden_size)\n",
    "        sample = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"frames\": frames,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        print(f\"[Dataset] Sample {idx}: placeholder positions {placeholder_positions.tolist()}\")\n",
    "        print(f\"[Dataset] input_ids: {input_ids.tolist()}\")\n",
    "        print(f\"[Dataset] labels: {labels.tolist()}\")\n",
    "        print(f\"[Dataset] frames shape: {frames.shape}\")\n",
    "        return sample\n",
    "\n",
    "def collate_batch(examples):\n",
    "    input_ids = torch.stack([ex[\"input_ids\"] for ex in examples])\n",
    "    frames = torch.stack([ex[\"frames\"] for ex in examples])\n",
    "    labels = torch.stack([ex[\"labels\"] for ex in examples])\n",
    "    print(f\"[collate] input_ids batch shape: {input_ids.shape}\")\n",
    "    print(f\"[collate] frames batch shape: {frames.shape}\")\n",
    "    print(f\"[collate] labels batch shape: {labels.shape}\")\n",
    "    return {\"input_ids\": input_ids, \"frames\": frames, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59dc632",
   "metadata": {},
   "source": [
    "## 5. Instantiate model and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98daa1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TinyLiveConfig()\n",
    "model = DummyLiveLlama(config)\n",
    "\n",
    "dataset = DummyVideoTextDataset(config)\n",
    "dataloader = DataLoader(dataset, batch_size=2, collate_fn=collate_batch, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a08d5e8",
   "metadata": {},
   "source": [
    "## 6. Single forward pass\n",
    "\n",
    "We run one batch through the model to observe how the embeddings and loss behave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d679b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))\n",
    "outputs = model(**batch)\n",
    "print(f\"Loss from single batch: {outputs['loss'].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70747e7a",
   "metadata": {},
   "source": [
    "## 7. Simple training loop\n",
    "\n",
    "Finally, we perform a short training loop over the dummy data.\n",
    "Gradient norms and loss values are printed each step to make the optimization dynamics explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a449f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-3)\n",
    "num_epochs = 2\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\n",
    "===== Epoch {epoch + 1}/{num_epochs} =====\")\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs[\"loss\"]\n",
    "        loss.backward()\n",
    "        total_norm = 0.0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = math.sqrt(total_norm)\n",
    "        print(f\"[train] step={step} loss={loss.item():.4f} grad_norm={total_norm:.4f}\")\n",
    "        optimizer.step()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
